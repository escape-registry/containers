name: Build, Validate, and Push Docker Recipes

on:
  push:
    branches:
      - main
    paths:
      - 'environments/**/**'
      - '.github/workflows/build-and-push.yml'
  pull_request:
    branches:
      - main
    paths:
      - 'environments/**/**'
      - '.github/workflows/build-and-push.yml'

permissions:
  contents: read
  packages: write # Required to push to GHCR
  pull-requests: write # Required to comment on PRs (optional)

jobs:
  build-and-validate-recipes:
    runs-on: ubuntu-latest
    outputs:
      changed_recipes_json: ${{ steps.identify-changes.outputs.changed_recipes_json }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Required for git diff to compare across full history

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install jq (for JSON processing)
        run: sudo apt-get update && sudo apt-get install -y jq

      - name: Install Dockerfile Linter (hadolint)
        run: |
          sudo wget -O /usr/local/bin/hadolint https://github.com/hadolint/hadolint/releases/download/v2.12.0/hadolint-Linux-x86_64
          sudo chmod +x /usr/local/bin/hadolint

      - name: Identify changed recipes (for PRs and main push)
        id: identify-changes
        run: |
          set -e # Exit immediately if a command fails, but be mindful of commands that can "fail" meaningfully (like grep not finding matches)
          echo "GITHUB_EVENT_NAME: ${{ github.event_name }}"
          echo "GITHUB_REF: ${{ github.ref }}"
          echo "GITHUB_SHA: ${{ github.sha }}"
          echo "GITHUB_BASE_REF: ${{ github.base_ref }}" # Populated for PRs
          echo "GITHUB_HEAD_REF: ${{ github.head_ref }}" # Populated for PRs
          echo "GITHUB_EVENT_BEFORE: ${{ github.event.before }}" # Populated for pushes

          changed_recipe_dirs_string=""
          raw_diff_files=""

          if [[ "${{ github.event_name }}" == "pull_request" ]]; then
            echo "Identifying changes for PR between ${{ github.base_ref }} and ${{ github.head_ref }}"
            # Ensure base branch is fetched if needed (fetch-depth: 0 should make it available)
            # git fetch origin ${{ github.base_ref }}:${{ github.base_ref }} --depth=1 # Usually not needed with fetch-depth:0
            
            raw_diff_files=$(git diff --name-only --diff-filter=AMR "origin/${{ github.base_ref }}" "${{ github.sha }}")
            if [ $? -ne 0 ]; then
                echo "::warning::git diff for PR failed. Raw diff files might be inaccurate."
            fi
            echo "Raw diff files for PR:"
            echo "${raw_diff_files}"
            
            if [[ -n "$raw_diff_files" ]]; then
              changed_recipe_dirs_string=$(echo "${raw_diff_files}" | grep '^environments/' | awk -F'/' '{print $1"/"$2"/"$3}' | sort -u)
            fi

          elif [[ "${{ github.event_name }}" == "push" && "${{ github.ref }}" == "refs/heads/main" ]]; then
            if [[ "${{ github.event.before }}" == "0000000000000000000000000000000000000000" ]]; then
              echo "New branch or first push to main. Identifying all environment directories."
              changed_recipe_dirs_string=$(find environments -mindepth 2 -maxdepth 2 -type d -print | sort -u)
            else
              echo "Identifying changes for push to main between ${{ github.event.before }} and ${{ github.sha }}"
              raw_diff_files=$(git diff --name-only --diff-filter=AMR "${{ github.event.before }}" "${{ github.sha }}")
              if [ $? -ne 0 ]; then
                  echo "::warning::git diff for push failed. Raw diff files might be inaccurate."
              fi
              echo "Raw diff files for push:"
              echo "${raw_diff_files}"

              if [[ -n "$raw_diff_files" ]]; then
                changed_recipe_dirs_string=$(echo "${raw_diff_files}" | grep '^environments/' | awk -F'/' '{print $1"/"$2"/"$3}' | sort -u)
              fi
            fi
          else
            echo "Unsupported event ('${{ github.event_name }}' on ref '${{ github.ref }}'). Building all recipes."
            changed_recipe_dirs_string=$(find environments -mindepth 2 -maxdepth 2 -type d -print | sort -u)
          fi

          if [[ -z "$changed_recipe_dirs_string" ]]; then
            echo "No modified or all environment directories identified for processing."
            echo "changed_recipes_json=[]" >> $GITHUB_OUTPUT
            exit 0
          fi

          echo "Identified recipe version directories to process:"
          echo "$changed_recipe_dirs_string"

          # Convert newline-separated string of directories into a JSON array
          json_array=$(echo "$changed_recipe_dirs_string" | jq --raw-input --slurp 'split("\n") | map(select(length > 0))')
          
          echo "Final JSON array of recipe paths: $json_array"
          # Remove leading/trailing whitespace from the JSON array string
          json_array=$(echo "$json_array" | tr -d '[:space:]')
          echo "changed_recipes_json=$json_array" >> $GITHUB_OUTPUT

  process-recipe:
    needs: build-and-validate-recipes
    if: needs.build-and-validate-recipes.outputs.changed_recipes_json != '[]' && needs.build-and-validate-recipes.outputs.changed_recipes_json != ''
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        recipe_path: ${{ fromJson(needs.build-and-validate-recipes.outputs.changed_recipes_json) }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      
      - name: Install Dockerfile Linter (hadolint)
        # This step is repeated from the previous job, consider making hadolint available globally or using a composite action if used often.
        run: |
          sudo wget -O /usr/local/bin/hadolint https://github.com/hadolint/hadolint/releases/download/v2.12.0/hadolint-Linux-x86_64
          sudo chmod +x /usr/local/bin/hadolint

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Extract Recipe Name and Version from Path
        id: extract-name-version
        run: |
          recipe_full_path="${{ matrix.recipe_path }}" # ex: environments/recipe-name/version
          echo "Processing recipe_path: $recipe_full_path"
          recipe_version_dir_name=$(basename "$recipe_full_path")
          recipe_name_dir=$(dirname "$recipe_full_path")
          recipe_name=$(basename "$recipe_name_dir")
          dockerfile_actual_path="${{ matrix.recipe_path }}/Dockerfile"

          echo "recipe_name=$recipe_name" >> $GITHUB_OUTPUT
          echo "recipe_version=$recipe_version_dir_name" >> $GITHUB_OUTPUT
          echo "dockerfile_path=$dockerfile_actual_path" >> $GITHUB_OUTPUT 
          echo "Recipe Name: $recipe_name, Version: $recipe_version_dir_name, Dockerfile: $dockerfile_actual_path"

      - name: Lint Dockerfile
        run: |
          echo "Linting Dockerfile: ${{ steps.extract-name-version.outputs.dockerfile_path }}"
          if [ ! -f "${{ steps.extract-name-version.outputs.dockerfile_path }}" ]; then
            echo "::error::Dockerfile not found at ${{ steps.extract-name-version.outputs.dockerfile_path }}"
            exit 1
          fi
          hadolint "${{ steps.extract-name-version.outputs.dockerfile_path }}"

      - name: Extract and Validate Metadata
        id: metadata
        run: |
          echo "Extracting and validating metadata for ${{ steps.extract-name-version.outputs.dockerfile_path }}"
          mkdir -p temp_metadata
          # Assuming extract_metadata.py can handle being called this way and outputs JSON
          # and that it exits non-zero on validation failure.
          python_script_path="scripts/extract_metadata.py" # Make sure this path is correct from repo root
          output_json_file="temp_metadata/${{ steps.extract-name-version.outputs.recipe_name }}-${{ steps.extract-name-version.outputs.recipe_version }}.json"
          
          python "$python_script_path" "${{ steps.extract-name-version.outputs.dockerfile_path }}" "$output_json_file"
          
          metadata_json_content=$(cat "$output_json_file")
          echo "metadata_json<<EOF" >> $GITHUB_OUTPUT
          echo "$metadata_json_content" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT
          echo "Metadata JSON content:"
          echo "$metadata_json_content"

      - name: Login to GitHub Container Registry
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build Docker Image
        id: build-image
        run: |
          DOCKERFILE_PATH="${{ steps.extract-name-version.outputs.dockerfile_path }}"
          RECIPE_NAME="${{ steps.extract-name-version.outputs.recipe_name }}"
          RECIPE_VERSION_FROM_PATH="${{ steps.extract-name-version.outputs.recipe_version }}"
          
          METADATA_JSON='${{ steps.metadata.outputs.metadata_json }}'
          # Ensure your metadata script outputs this label key correctly
          # Also, ensure your Dockerfile labels use this exact key (e.g., org.escape-registry.recipe.version)
          LABEL_VERSION=$(echo "$METADATA_JSON" | jq -r '.["org.escape-registry.recipe.version"] // .["org.yourproject.recipe.version"]') # Adapt if your prefix changed

          if [ -z "$LABEL_VERSION" ] || [ "$LABEL_VERSION" == "null" ]; then
            echo "::error::Could not extract 'org.escape-registry.recipe.version' or 'org.yourproject.recipe.version' from metadata JSON."
            echo "Metadata JSON: $METADATA_JSON"
            exit 1
          fi

          # It's good practice to ensure directory version matches label version
          if [ "$LABEL_VERSION" != "$RECIPE_VERSION_FROM_PATH" ]; then
            echo "::warning::Label version ($LABEL_VERSION) does not match directory version ($RECIPE_VERSION_FROM_PATH) for $RECIPE_NAME. Using label version for tagging."
            # Decide if this should be an error: exit 1
          fi

          IMAGE_OWNER=$(echo "${{ github.repository_owner }}" | tr '[:upper:]' '[:lower:]')
          IMAGE_REPO_NAME=$(echo "${{ github.event.repository.name }}" | tr '[:upper:]' '[:lower:]')
          IMAGE_RECIPE_NAME=$(echo "$RECIPE_NAME" | tr '[:upper:]' '[:lower:]')

          IMAGE_NAME_BASE="ghcr.io/$IMAGE_OWNER/$IMAGE_REPO_NAME/$IMAGE_RECIPE_NAME"
          
          TAG_VERSION="$IMAGE_NAME_BASE:$LABEL_VERSION"
          # A common strategy for 'latest' is to tag the highest semantic version.
          # For simplicity here, 'latest' will point to this specific build if it's on main.
          # More complex 'latest' handling might involve comparing versions.
          TAG_LATEST="$IMAGE_NAME_BASE:latest" 
          TAG_SHA="$IMAGE_NAME_BASE:sha-${GITHUB_SHA::7}"

          echo "Building image $IMAGE_RECIPE_NAME version $LABEL_VERSION with Dockerfile $DOCKERFILE_PATH"
          echo "Tags to be applied: $TAG_VERSION, $TAG_LATEST (if main branch), $TAG_SHA"

          BUILD_ARGS_JSON=$(echo "$METADATA_JSON" | jq -r '.["org.escape-registry.recipe.build_arguments"] // .["org.yourproject.recipe.build_arguments"] // ""') # Adapt prefix
          BUILD_ARGS_STRING=""
          if [[ -n "$BUILD_ARGS_JSON" && "$BUILD_ARGS_JSON" != "null" && "$BUILD_ARGS_JSON" != "" ]]; then
            echo "Build arguments detected: $BUILD_ARGS_JSON"
            BUILD_ARGS_STRING=$(echo "$BUILD_ARGS_JSON" | jq -r 'to_entries | .[] | "--build-arg \(.key)=\"\(.value)\""' | tr '\n' ' ')
            echo "Build arguments string: $BUILD_ARGS_STRING"
          fi
          
          BUILD_CONTEXT_DIR=$(dirname "$DOCKERFILE_PATH")
          echo "Build context directory: $BUILD_CONTEXT_DIR"

          docker buildx build --platform linux/amd64 -t "$TAG_VERSION" -t "$TAG_SHA" $BUILD_ARGS_STRING -f "$DOCKERFILE_PATH" "$BUILD_CONTEXT_DIR" --load
          
          if [[ "${{ github.event_name }}" == "push" && "${{ github.ref }}" == "refs/heads/main" ]]; then
            docker tag "$TAG_VERSION" "$TAG_LATEST"
          fi

          echo "image_tag_version=$TAG_VERSION" >> $GITHUB_OUTPUT
          echo "image_tag_latest=$TAG_LATEST" >> $GITHUB_OUTPUT
          echo "image_tag_sha=$TAG_SHA" >> $GITHUB_OUTPUT

      - name: Push Docker Image to GHCR
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          echo "Pushing image ${{ steps.build-image.outputs.image_tag_version }}"
          docker push "${{ steps.build-image.outputs.image_tag_version }}"
          
          echo "Pushing image ${{ steps.build-image.outputs.image_tag_latest }}"
          docker push "${{ steps.build-image.outputs.image_tag_latest }}"
          
          echo "Pushing image ${{ steps.build-image.outputs.image_tag_sha }}"
          docker push "${{ steps.build-image.outputs.image_tag_sha }}"

  update-github-pages-data:
    needs: [process-recipe] # Runs after all modified recipes have been processed (or attempted)
    if: success() && github.event_name == 'push' && github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
      
      - name: Install jq
        run: sudo apt-get update && sudo apt-get install -y jq

      - name: Aggregate metadata from all Dockerfiles
        id: aggregate-metadata
        run: |
          echo "Aggregating metadata from all Dockerfiles for GitHub Pages..."
          output_json_file="docs/_data/environments.json"
          mkdir -p "$(dirname "$output_json_file")"

          all_dockerfiles=$(find environments -type f -name Dockerfile)
          json_entries=()

          if [ -z "$all_dockerfiles" ]; then
            echo "No Dockerfiles found. Creating an empty environments.json file."
            echo "[]" > "$output_json_file"
          else
            for df_path in $all_dockerfiles; do
              echo "Processing $df_path for aggregation..."
              # Assuming extract_metadata.py prints JSON to stdout if no output file is given
              # AND it includes _recipe_name and _recipe_version_from_path
              metadata_entry_json=$(python scripts/extract_metadata.py "$df_path")
              if [ $? -eq 0 ] && [ -n "$metadata_entry_json" ] && [ "$metadata_entry_json" != "null" ]; then
                json_entries+=("$metadata_entry_json")
              else
                echo "::warning::Failed to extract metadata from $df_path or metadata was empty/null."
              fi
            done
            
            # Combine all JSON objects into a single JSON array
            # Using jq to build the array safely
            printf '%s\n' "${json_entries[@]}" | jq -s '.' > "$output_json_file"
          fi

          echo "Aggregated metadata saved in $output_json_file"
          echo "Content of $output_json_file:"
          cat "$output_json_file"
          echo "aggregated_json_path=$output_json_file" >> $GITHUB_OUTPUT

      - name: Commit and push GitHub Pages data
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "docs: Update environment data for GitHub Pages"
          branch: main # Or your GitHub Pages branch if different
          file_pattern: "docs/_data/environments.json"
          # repository: . # Usually not needed
          # push_options: '--force' # Use with caution
